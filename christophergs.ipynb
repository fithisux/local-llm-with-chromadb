{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fc5a45-b7bd-4ef0-b4a2-1bd89bdef6a9",
   "metadata": {},
   "source": [
    "From [Christorpher GS Blog](https://christophergs.com/blog/ai-engineering-retrieval-augmented-generation-rag-llama-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe852d2b-8a02-40cb-8ff2-e6efb2611b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'curl' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# get model\n",
    "\n",
    "!curl https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305fb7ad-2745-4a2d-bade-839283f1c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from ./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q4_K:  833 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 24.62 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors:        CPU buffer size = 25215.87 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32000\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    71.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2052.03 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_path=\"./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",  # https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\n",
    "    context_window=32000,\n",
    "    max_new_tokens=1024,\n",
    "    #model_kwargs={'n_gpu_layers': 1},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"WhereIsAI/UAE-Large-V1\")  # https://huggingface.co/WhereIsAI/UAE-Large-V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bacda66-39d8-4217-a27b-1274f667f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b63b6b-efd2-4388-833e-316c23cbe6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from llama_index.core import set_global_handler\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "set_global_handler(\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad13261-8a05-4131-aafe-ad5a33cb7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VasilisAnagnostopoul\\AppData\\Local\\Temp\\ipykernel_1500\\3950332740.py:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_model,\n",
    "    system_prompt='You are a bot that answers questions about podcast transcripts'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13db9d1-231e-4455-a5a0-e8cac1f2c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99980e8-2254-4f96-b438-32a515e8dec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2066c136232f431884bcf1d158f4b5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Hey everyone, welcome to the Latent Space Podca...\n",
      "> Adding chunk: Hey everyone, welcome to the Latent Space Podca...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: But of course there's data and all this sort of...\n",
      "> Adding chunk: But of course there's data and all this sort of...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I actually only have access to gaming GPUs.\n",
      "Sho...\n",
      "> Adding chunk: I actually only have access to gaming GPUs.\n",
      "Sho...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: How do you see that changing, now that a lot of...\n",
      "> Adding chunk: How do you see that changing, now that a lot of...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Like people train like 2 million batch sizes is...\n",
      "> Adding chunk: Like people train like 2 million batch sizes is...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I would argue 30 milliseconds per token.\n",
      "Some p...\n",
      "> Adding chunk: I would argue 30 milliseconds per token.\n",
      "Some p...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: They've done a ton there.\n",
      "Obviously Google work...\n",
      "> Adding chunk: They've done a ton there.\n",
      "Obviously Google work...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: To run Llama 70 billion requires 2 terabytes a ...\n",
      "> Adding chunk: To run Llama 70 billion requires 2 terabytes a ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: If you buy an H100, sure, the next series is go...\n",
      "> Adding chunk: If you buy an H100, sure, the next series is go...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Open source will match GPT-4, but then it's lik...\n",
      "> Adding chunk: Open source will match GPT-4, but then it's lik...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Same applies to Samanova, same applies to Cereb...\n",
      "> Adding chunk: Same applies to Samanova, same applies to Cereb...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: So it's like, you know, it's tough for anyone t...\n",
      "> Adding chunk: So it's like, you know, it's tough for anyone t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: And it's like, obviously you're competing and M...\n",
      "> Adding chunk: And it's like, obviously you're competing and M...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Maybe the SemiAnalysis analyst point of view is...\n",
      "> Adding chunk: Maybe the SemiAnalysis analyst point of view is...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Kind of like, you know, in a different sense, l...\n",
      "> Adding chunk: Kind of like, you know, in a different sense, l...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I'm a fellow writer on Substack.\n",
      "You are obviou...\n",
      "> Adding chunk: I'm a fellow writer on Substack.\n",
      "You are obviou...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Then there's maybe some unknowns where you're l...\n",
      "> Adding chunk: Then there's maybe some unknowns where you're l...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e004e06a7744c9ea7eb0e30a45229e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcript_directory = '.'\n",
    "# Find all files that match the pattern \"*transcript*\" using the Python standard library glob module\n",
    "transcript_files = glob.glob(str(Path(transcript_directory) / '**/transcript44.txt'), recursive=True)\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=transcript_files).load_data()\n",
    "\n",
    "# We pass in the service context we instantiated earlier (powered by our open-source LLM)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ecf515-ffa3-4b83-bb3e-f3209f1449c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  103084.24 ms\n",
      "llama_print_timings:      sample time =      80.36 ms /   249 runs   (    0.32 ms per token,  3098.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  103083.17 ms /    22 tokens ( 4685.60 ms per token,     0.21 tokens per second)\n",
      "llama_print_timings:        eval time =  637223.07 ms /   248 runs   ( 2569.45 ms per token,     0.39 tokens per second)\n",
      "llama_print_timings:       total time =  744039.33 ms /   270 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prompt: **\n",
      "According to Dylan Patel, what don't people understand about the semiconductor supply chain?\n",
      "**************************************************\n",
      "** Completion: **\n",
      "\n",
      "\n",
      "Dylan Patel: I think one of the things that people don't understand is how long it takes for a new fab to be built. It can take anywhere from two to four years just to build the fab and then another six months to a year to get it fully qualified and up and running at full capacity. So when you see these headlines saying there's going to be a shortage in 2023 or 2024, it's not because there's not enough capacity being added. It's because it takes so long for that capacity to come online. And then once it does come online, it's not like you can just flip a switch and suddenly have all this capacity available. It has to be ramped up slowly over time. So when people see these headlines and they think, \"Oh, there's going to be a shortage in 2023 or 2024,\" they might think, \"Well, why don't we just build more fabs?\" But it's not that simple. It takes a long time for those fabs to come online and then even longer to ramp up the capacity.\n",
      "**************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dylan Patel: I think one of the things that people don't understand is how long it takes for a new fab to be built. It can take anywhere from two to four years just to build the fab and then another six months to a year to get it fully qualified and up and running at full capacity. So when you see these headlines saying there's going to be a shortage in 2023 or 2024, it's not because there's not enough capacity being added. It's because it takes so long for that capacity to come online. And then once it does come online, it's not like you can just flip a switch and suddenly have all this capacity available. It has to be ramped up slowly over time. So when people see these headlines and they think, \"Oh, there's going to be a shortage in 2023 or 2024,\" they might think, \"Well, why don't we just build more fabs?\" But it's not that simple. It takes a long time for those fabs to come online and then even longer to ramp up the capacity.\n"
     ]
    }
   ],
   "source": [
    "#just ask LamaCPP\n",
    "response = llm.complete(\"According to Dylan Patel, what don't people understand about the semiconductor supply chain?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b511931d-a234-4bc5-a9d0-817eb753e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node 718659e3-fd6d-4d66-a9cd-e2276df548bf] [Similarity score:             0.653151] I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're...\n",
      "> [Node 6df9d824-3260-47eb-bc18-49e4066f81c6] [Similarity score:             0.620988] Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the ...\n",
      "> Top 2 nodes:\n",
      "> [Node 718659e3-fd6d-4d66-a9cd-e2276df548bf] [Similarity score:             0.653151] I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're...\n",
      "> [Node 6df9d824-3260-47eb-bc18-49e4066f81c6] [Similarity score:             0.620988] Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  103084.24 ms\n",
      "llama_print_timings:      sample time =      50.30 ms /   127 runs   (    0.40 ms per token,  2524.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  577741.87 ms /  1029 tokens (  561.46 ms per token,     1.78 tokens per second)\n",
      "llama_print_timings:        eval time =  364217.23 ms /   126 runs   ( 2890.61 ms per token,     0.35 tokens per second)\n",
      "llama_print_timings:       total time =  943574.16 ms /  1155 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prompt: **\n",
      "You are a bot that answers questions about podcast transcripts\n",
      "\n",
      "Context information is below.\n",
      "---------------------\n",
      "file_path: transcript\\transcript44.txt\n",
      "\n",
      "I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're also publishing these amazing posts.\n",
      "What's your writing process?\n",
      "How do you source info?\n",
      "When do you sit down and go, like, here's the theme for the week?\n",
      "Do you have a pipeline going out?\n",
      "Just anything you can describe.\n",
      "I'm thankful for my teammates because they are actually awesome, and they're much more directed, focused, working on one thing.\n",
      "Or not one thing, but a number of things, like someone who's an expert on X and Y and Z and the semiconductor supply chain.\n",
      "So that really helps with that side of the business.\n",
      "I, most of the times, only write when I'm very excited, or it's like, hey, we should work on this and we should write about this.\n",
      "One of the most recent posts we did was we explained the manufacturing process for 3D NAND, flash storage, gate all-around transistors, and 3D DRAM and all this sort of stuff, because there's a company in Japan that's going public, Kokusai Electric, right?\n",
      "It was like, okay, well, we should do a post about this, and we should explain this.\n",
      "Dylan Patel of SemiAnalysis\n",
      "There's stuff like that that we do and that like builds up a body of work for our consulting and some of the reports that we sell that aren't, you know, newsletter posts.\n",
      "But a lot of times the process is also just like, well, like Mina Eats the World is the culmination of reading that, having done a lot of work on the supply chain around the TPU ramp and coasts and HBM capacities and all this sort of stuff to be able to, you know, figure out how many units and that Google's ordering all sort of stuff.\n",
      "And then like, also like looking at like open sources, like all just that all that culminated in like, I wrote that in four hours, right?\n",
      "and Dylan\n",
      "you know obviously like what was in the Gemini Eats the World post you know obviously hey like we we do deep work there's a lot more like factual not leaks you know it's just factual research hey we across the team we go to 40 plus conferences a year right all the way from like a photoresist conference to photomask conference to a lithography conference all the way up to like AI conferences and you know all everything in between networking conferences and piecing everything across the supply chain so it's like that's like the true like\n",
      "work and like yeah i don't know it is sometimes bad to like have the infamousness of you know only people caring about this or the gp4 leak or the google has no mote leak right it's like but like you know that's just like stuff that comes along right you know it's really focused on like understanding the supply chain and how it's pivoting and who's the winners who's the losers what technologies are inflecting things like that where is the best place to invest resources you know sort of like stuff like that and accelerating or capturing value etc\n",
      "Awesome.\n",
      "And to wrap, if you had a magic genie that could answer any question that would change your worldview, what question would you ask?\n",
      "That's a tough one.\n",
      "Like, you operate based on a set of facts about the world right now.\n",
      "Then there's maybe some unknowns where you're like, man, if I really knew the answer to this one, I would do so many things differently, or I would think about things very differently.\n",
      "So I'm of the view, at least everything that we've seen so far, is that large scale training has to happen in an individual data center with very high speed networking.\n",
      "Now, everything doesn't need to be all to all connected, but you need very high speed networking between all of your, your chips, right?\n",
      "I would love to know\n",
      "Hey, Magic Genie, how can we build artificial intelligence in a way that it can use multiple data centers of resources where there is a significantly lower bandwidth between pools of resources?\n",
      "One of the big bottlenecks is how much power and how many chips you can get into a single data center.\n",
      "\n",
      "file_path: transcript\\transcript44.txt\n",
      "\n",
      "Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the US?\n",
      "The State of Silicon and the\n",
      "You know Austria has two companies like the country of Austria and Europe has two companies that have super high market share and very specific technologies that are required for every single like like chip period right there is no chip that is less than seven nanometer that doesn't get touched by this one Austrian company's tool right and there is no alternative\n",
      "And there's another Austrian company, likewise, everything 2nm and beyond will be touched by their tool.\n",
      "It's like both of these companies are doing well, less than a billion dollars in revenue, right?\n",
      "So it's like, you think it's so inconsequential.\n",
      "No, there's actually like three or four Japanese chemical companies.\n",
      "Same idea, right?\n",
      "It's like, the supply chain is so fragmented, right?\n",
      "Like, people only ever talk about where the fabs, where they actually get produced.\n",
      "But it's like, I mean, TSMC in Arizona, right?\n",
      "TSMC is building a fab in Arizona.\n",
      "It's quite a bit smaller than the fabs in Taiwan.\n",
      "But even ignoring that, those fabs still have to ship everything to Taiwan back anyways.\n",
      "And also, they have to get what's called a mask\n",
      "from Taiwan to Arizona\n",
      "Re-engineer and rebuild it on a snap, right?\n",
      "It's just complex to do that.\n",
      "Semiconductors are more complex than any other thing that humans do, without a doubt.\n",
      "There's more people working in that supply chain with XYZ backgrounds and more money invested every year and R&D plus CapEx, you know.\n",
      "It's just by far the most complex supply chain that humanity has.\n",
      "And to think that we could rebuild it in a few years is absurd.\n",
      "In the alternate universe, the U.S.\n",
      "kept Morris Chang.\n",
      "I mean, he built it here, right?\n",
      "It was just one guy.\n",
      "Yeah, in an alternative universe, Texas Instruments communicated to Morris Chang that he would become CEO and so he never goes to Taiwan and you know, blah, blah, blah, right?\n",
      "Yeah, no, but I, you know, that's just also, I think, I think the world would probably be further behind in terms of technology development if that didn't happen, right?\n",
      "The State of Silicon and the GPU Poors\n",
      "Let's get a quick lightning round done.\n",
      "SemiAnalysis branded one.\n",
      "So the first one is what are like foundational readings that people that are listening today should read to get up to speed on like semis?\n",
      "I think the easiest one is the PyTorch 2.0 and Triton one that I did.\n",
      "You know, there's the advanced packaging series.\n",
      "There's the Google infrastructure supremacy piece.\n",
      "I think that one's really critical because it explains Google's infrastructure quite a bit from networking through chips, through all that sort of history of the TPU a little bit.\n",
      "Maybe like AMD's MI300 piece talks a lot about the one that we did on that are very good.\n",
      "And then obviously like, you know, like, I don't know, probably like Chip Wars by Chris Miller, who doesn't recommend that book, right?\n",
      "It's a really good book, right?\n",
      "I mean, like, I would say Gordon Moore's book is freaking awesome, because you got to think about, right, like, you know, LLM scaling laws are like Moore's law on crack, right?\n",
      "Kind of like, you know, in a different sense, like, you know, if you think about all of human productivity gains since the 70s is probably just off of the base of semiconductors and technology, right?\n",
      "Of course, of course, people across the world are getting, you know, access to oil and gas and all this sort of stuff.\n",
      "But like,\n",
      "At least in the Western world, since the 70s, everything has just been mostly innovated because of technology, right?\n",
      "Oh, we're able to build better cars because semiconductors enable us to do that.\n",
      "Or we're able to build better software because we're able to connect everyone because semiconductors enabled that, right?\n",
      "So it's like, that is like, I think that's why it's the most important industry in the world.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: According to Dylan Patel, what don't people understand about the semiconductor supply chain?\n",
      "Answer: \n",
      "**************************************************\n",
      "** Completion: **\n",
      "\n",
      "Dylan Patel of SemiAnalysis believes that people don't understand the complexity and fragmentation of the semiconductor supply chain beyond the location of the fabs where the chips are produced. He emphasizes the importance of other companies in the supply chain, such as the Austrian companies with high market share and specific technologies required for chip production, and the Japanese chemical companies involved in the process. He also mentions the challenge of building artificial intelligence to use multiple data centers with lower bandwidth between pools of resources, and the complexity of rebuilding the semiconductor supply chain in a few years.\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ask RAG, is it any better???\n",
    "query_engine = index.as_query_engine()\n",
    "result = query_engine.query(\"According to Dylan Patel, what don't people understand about the semiconductor supply chain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd77c69-be0c-49e6-9572-5ed2ade65d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDylan Patel of SemiAnalysis believes that people don't understand the complexity and fragmentation of the semiconductor supply chain beyond the location of the fabs where the chips are produced. He emphasizes the importance of other companies in the supply chain, such as the Austrian companies with high market share and specific technologies required for chip production, and the Japanese chemical companies involved in the process. He also mentions the challenge of building artificial intelligence to use multiple data centers with lower bandwidth between pools of resources, and the complexity of rebuilding the semiconductor supply chain in a few years.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8da2bf-1046-40c7-8d7b-9252c9ee4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Hey everyone, welcome to the Latent Space Podca...\n",
      "> Adding chunk: Hey everyone, welcome to the Latent Space Podca...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: But of course there's data and all this sort of...\n",
      "> Adding chunk: But of course there's data and all this sort of...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I actually only have access to gaming GPUs.\n",
      "Sho...\n",
      "> Adding chunk: I actually only have access to gaming GPUs.\n",
      "Sho...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: How do you see that changing, now that a lot of...\n",
      "> Adding chunk: How do you see that changing, now that a lot of...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Like people train like 2 million batch sizes is...\n",
      "> Adding chunk: Like people train like 2 million batch sizes is...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I would argue 30 milliseconds per token.\n",
      "Some p...\n",
      "> Adding chunk: I would argue 30 milliseconds per token.\n",
      "Some p...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: They've done a ton there.\n",
      "Obviously Google work...\n",
      "> Adding chunk: They've done a ton there.\n",
      "Obviously Google work...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: To run Llama 70 billion requires 2 terabytes a ...\n",
      "> Adding chunk: To run Llama 70 billion requires 2 terabytes a ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: If you buy an H100, sure, the next series is go...\n",
      "> Adding chunk: If you buy an H100, sure, the next series is go...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Open source will match GPT-4, but then it's lik...\n",
      "> Adding chunk: Open source will match GPT-4, but then it's lik...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Same applies to Samanova, same applies to Cereb...\n",
      "> Adding chunk: Same applies to Samanova, same applies to Cereb...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: So it's like, you know, it's tough for anyone t...\n",
      "> Adding chunk: So it's like, you know, it's tough for anyone t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: And it's like, obviously you're competing and M...\n",
      "> Adding chunk: And it's like, obviously you're competing and M...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Maybe the SemiAnalysis analyst point of view is...\n",
      "> Adding chunk: Maybe the SemiAnalysis analyst point of view is...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Kind of like, you know, in a different sense, l...\n",
      "> Adding chunk: Kind of like, you know, in a different sense, l...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I'm a fellow writer on Substack.\n",
      "You are obviou...\n",
      "> Adding chunk: I'm a fellow writer on Substack.\n",
      "You are obviou...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Then there's maybe some unknowns where you're l...\n",
      "> Adding chunk: Then there's maybe some unknowns where you're l...\n",
      "INFO:backoff:Backing off send_request(...) for 0.6s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33A9FC10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.6s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33A9FC10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (2): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (2): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 1.3s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F3340B950>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 1.3s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F3340B950>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (3): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (3): us-api.i.posthog.com:443\n",
      "DEBUG:chromadb.config:Starting component LocalHnswSegment\n",
      "Starting component LocalHnswSegment\n",
      "DEBUG:llama_index.vector_stores.chroma.base:> Top 1 nodes:\n",
      "> Top 1 nodes:\n",
      "DEBUG:llama_index.vector_stores.chroma.base:> [Node 0969d8ca-44ba-4d4f-9d71-e07204b2e944] [Similarity score: 0.4997247609777877] I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're...\n",
      "> [Node 0969d8ca-44ba-4d4f-9d71-e07204b2e944] [Similarity score: 0.4997247609777877] I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're...\n",
      "DEBUG:llama_index.vector_stores.chroma.base:> [Node d8d484e3-d76c-4954-b882-e728d4d6f938] [Similarity score: 0.46859126462591083] Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the ...\n",
      "> [Node d8d484e3-d76c-4954-b882-e728d4d6f938] [Similarity score: 0.46859126462591083] Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the ...\n",
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node 0969d8ca-44ba-4d4f-9d71-e07204b2e944] [Similarity score:             0.499725] I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're...\n",
      "> [Node d8d484e3-d76c-4954-b882-e728d4d6f938] [Similarity score:             0.468591] Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the ...\n",
      "> Top 2 nodes:\n",
      "> [Node 0969d8ca-44ba-4d4f-9d71-e07204b2e944] [Similarity score:             0.499725] I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're...\n",
      "> [Node d8d484e3-d76c-4954-b882-e728d4d6f938] [Similarity score:             0.468591] Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off send_request(...) for 1.3s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F3276CB90>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 1.3s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F3276CB90>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (4): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (4): us-api.i.posthog.com:443\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33396B10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33396B10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (5): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (5): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.9s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F27CB6FD0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.9s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F27CB6FD0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (6): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (6): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.9s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F2AD6EE10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.9s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F2AD6EE10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (7): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (7): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 1.9s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F26F0AED0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 1.9s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F26F0AED0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (8): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (8): us-api.i.posthog.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  103084.24 ms\n",
      "llama_print_timings:      sample time =     113.11 ms /   138 runs   (    0.82 ms per token,  1220.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  223537.33 ms /   138 runs   ( 1619.84 ms per token,     0.62 tokens per second)\n",
      "llama_print_timings:       total time =  230992.71 ms /   139 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prompt: **\n",
      "You are a bot that answers questions about podcast transcripts\n",
      "\n",
      "Context information is below.\n",
      "---------------------\n",
      "file_path: transcript\\transcript44.txt\n",
      "\n",
      "I'm a fellow writer on Substack.\n",
      "You are obviously managing your consulting business while you're also publishing these amazing posts.\n",
      "What's your writing process?\n",
      "How do you source info?\n",
      "When do you sit down and go, like, here's the theme for the week?\n",
      "Do you have a pipeline going out?\n",
      "Just anything you can describe.\n",
      "I'm thankful for my teammates because they are actually awesome, and they're much more directed, focused, working on one thing.\n",
      "Or not one thing, but a number of things, like someone who's an expert on X and Y and Z and the semiconductor supply chain.\n",
      "So that really helps with that side of the business.\n",
      "I, most of the times, only write when I'm very excited, or it's like, hey, we should work on this and we should write about this.\n",
      "One of the most recent posts we did was we explained the manufacturing process for 3D NAND, flash storage, gate all-around transistors, and 3D DRAM and all this sort of stuff, because there's a company in Japan that's going public, Kokusai Electric, right?\n",
      "It was like, okay, well, we should do a post about this, and we should explain this.\n",
      "Dylan Patel of SemiAnalysis\n",
      "There's stuff like that that we do and that like builds up a body of work for our consulting and some of the reports that we sell that aren't, you know, newsletter posts.\n",
      "But a lot of times the process is also just like, well, like Mina Eats the World is the culmination of reading that, having done a lot of work on the supply chain around the TPU ramp and coasts and HBM capacities and all this sort of stuff to be able to, you know, figure out how many units and that Google's ordering all sort of stuff.\n",
      "And then like, also like looking at like open sources, like all just that all that culminated in like, I wrote that in four hours, right?\n",
      "and Dylan\n",
      "you know obviously like what was in the Gemini Eats the World post you know obviously hey like we we do deep work there's a lot more like factual not leaks you know it's just factual research hey we across the team we go to 40 plus conferences a year right all the way from like a photoresist conference to photomask conference to a lithography conference all the way up to like AI conferences and you know all everything in between networking conferences and piecing everything across the supply chain so it's like that's like the true like\n",
      "work and like yeah i don't know it is sometimes bad to like have the infamousness of you know only people caring about this or the gp4 leak or the google has no mote leak right it's like but like you know that's just like stuff that comes along right you know it's really focused on like understanding the supply chain and how it's pivoting and who's the winners who's the losers what technologies are inflecting things like that where is the best place to invest resources you know sort of like stuff like that and accelerating or capturing value etc\n",
      "Awesome.\n",
      "And to wrap, if you had a magic genie that could answer any question that would change your worldview, what question would you ask?\n",
      "That's a tough one.\n",
      "Like, you operate based on a set of facts about the world right now.\n",
      "Then there's maybe some unknowns where you're like, man, if I really knew the answer to this one, I would do so many things differently, or I would think about things very differently.\n",
      "So I'm of the view, at least everything that we've seen so far, is that large scale training has to happen in an individual data center with very high speed networking.\n",
      "Now, everything doesn't need to be all to all connected, but you need very high speed networking between all of your, your chips, right?\n",
      "I would love to know\n",
      "Hey, Magic Genie, how can we build artificial intelligence in a way that it can use multiple data centers of resources where there is a significantly lower bandwidth between pools of resources?\n",
      "One of the big bottlenecks is how much power and how many chips you can get into a single data center.\n",
      "\n",
      "file_path: transcript\\transcript44.txt\n",
      "\n",
      "Maybe the SemiAnalysis analyst point of view is, is it feasible to build this capacity up in the US?\n",
      "The State of Silicon and the\n",
      "You know Austria has two companies like the country of Austria and Europe has two companies that have super high market share and very specific technologies that are required for every single like like chip period right there is no chip that is less than seven nanometer that doesn't get touched by this one Austrian company's tool right and there is no alternative\n",
      "And there's another Austrian company, likewise, everything 2nm and beyond will be touched by their tool.\n",
      "It's like both of these companies are doing well, less than a billion dollars in revenue, right?\n",
      "So it's like, you think it's so inconsequential.\n",
      "No, there's actually like three or four Japanese chemical companies.\n",
      "Same idea, right?\n",
      "It's like, the supply chain is so fragmented, right?\n",
      "Like, people only ever talk about where the fabs, where they actually get produced.\n",
      "But it's like, I mean, TSMC in Arizona, right?\n",
      "TSMC is building a fab in Arizona.\n",
      "It's quite a bit smaller than the fabs in Taiwan.\n",
      "But even ignoring that, those fabs still have to ship everything to Taiwan back anyways.\n",
      "And also, they have to get what's called a mask\n",
      "from Taiwan to Arizona\n",
      "Re-engineer and rebuild it on a snap, right?\n",
      "It's just complex to do that.\n",
      "Semiconductors are more complex than any other thing that humans do, without a doubt.\n",
      "There's more people working in that supply chain with XYZ backgrounds and more money invested every year and R&D plus CapEx, you know.\n",
      "It's just by far the most complex supply chain that humanity has.\n",
      "And to think that we could rebuild it in a few years is absurd.\n",
      "In the alternate universe, the U.S.\n",
      "kept Morris Chang.\n",
      "I mean, he built it here, right?\n",
      "It was just one guy.\n",
      "Yeah, in an alternative universe, Texas Instruments communicated to Morris Chang that he would become CEO and so he never goes to Taiwan and you know, blah, blah, blah, right?\n",
      "Yeah, no, but I, you know, that's just also, I think, I think the world would probably be further behind in terms of technology development if that didn't happen, right?\n",
      "The State of Silicon and the GPU Poors\n",
      "Let's get a quick lightning round done.\n",
      "SemiAnalysis branded one.\n",
      "So the first one is what are like foundational readings that people that are listening today should read to get up to speed on like semis?\n",
      "I think the easiest one is the PyTorch 2.0 and Triton one that I did.\n",
      "You know, there's the advanced packaging series.\n",
      "There's the Google infrastructure supremacy piece.\n",
      "I think that one's really critical because it explains Google's infrastructure quite a bit from networking through chips, through all that sort of history of the TPU a little bit.\n",
      "Maybe like AMD's MI300 piece talks a lot about the one that we did on that are very good.\n",
      "And then obviously like, you know, like, I don't know, probably like Chip Wars by Chris Miller, who doesn't recommend that book, right?\n",
      "It's a really good book, right?\n",
      "I mean, like, I would say Gordon Moore's book is freaking awesome, because you got to think about, right, like, you know, LLM scaling laws are like Moore's law on crack, right?\n",
      "Kind of like, you know, in a different sense, like, you know, if you think about all of human productivity gains since the 70s is probably just off of the base of semiconductors and technology, right?\n",
      "Of course, of course, people across the world are getting, you know, access to oil and gas and all this sort of stuff.\n",
      "But like,\n",
      "At least in the Western world, since the 70s, everything has just been mostly innovated because of technology, right?\n",
      "Oh, we're able to build better cars because semiconductors enable us to do that.\n",
      "Or we're able to build better software because we're able to connect everyone because semiconductors enabled that, right?\n",
      "So it's like, that is like, I think that's why it's the most important industry in the world.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: According to Dylan Patel, what don't people understand about the semiconductor supply chain?\n",
      "Answer: \n",
      "**************************************************\n",
      "** Completion: **\n",
      "\n",
      "Dylan Patel of SemiAnalysis believes that people often overlook the complexity and fragmentation of the semiconductor supply chain beyond just the location of fabs. He mentions the importance of tools and materials provided by companies in countries like Austria and Japan, which are essential for chip production at various nodes. Additionally, he highlights the challenge of shipping masks and other components between data centers with lower bandwidth connections, as well as the time and resources required to rebuild and re-engineer these connections. He emphasizes the need for high-speed networking between chips in a single data center and the difficulty of replicating the entire supply chain in a few years.\n",
      "**************************************************\n",
      "\n",
      "\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F34DECB90>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F34DECB90>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n"
     ]
    }
   ],
   "source": [
    "#try with chroma db\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store2 = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context2 = StorageContext.from_defaults(vector_store=vector_store2)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context2, embed_model=embedding_model, service_context=service_context\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"According to Dylan Patel, what don't people understand about the semiconductor supply chain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d528b4a9-b1b6-43c3-8f3b-c148b9043985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: \n",
      "> Adding chunk: \n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Trial\n",
      "by\n",
      "Sorcery\n",
      "\t\n",
      "Dragon\tRiders\tof\tOsnen\tBook\t...\n",
      "> Adding chunk: Trial\n",
      "by\n",
      "Sorcery\n",
      "\t\n",
      "Dragon\tRiders\tof\tOsnen\tBook\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Trial\tby\tSorcery\t©\t2020\tby\tRichard\tFierce\n",
      "\t\n",
      "\t\n",
      "T...\n",
      "> Adding chunk: Trial\tby\tSorcery\t©\t2020\tby\tRichard\tFierce\n",
      "\t\n",
      "\t\n",
      "T...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: TABLE\tOF\tCONTENTS\n",
      "Chapter\t1\n",
      "Chapter\t2\n",
      "Chapter\t3...\n",
      "> Adding chunk: TABLE\tOF\tCONTENTS\n",
      "Chapter\t1\n",
      "Chapter\t2\n",
      "Chapter\t3...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "...\n",
      "> Adding chunk: 1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: least\ta\thundred\tpeople\twaiting\tto\tget\tinto\tthe\t...\n",
      "> Adding chunk: least\ta\thundred\tpeople\twaiting\tto\tget\tinto\tthe\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I\tnodded\tat\thim,\tstill\tsmiling,\tand\twalked\tover...\n",
      "> Adding chunk: I\tnodded\tat\thim,\tstill\tsmiling,\tand\twalked\tover...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: giving\tme\ta\tdeath\tstare.\n",
      "“Did\tI\task\tfor\tyour\the...\n",
      "> Adding chunk: giving\tme\ta\tdeath\tstare.\n",
      "“Did\tI\task\tfor\tyour\the...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Don’t\tworry\tabout\tit,”\tI\tsaid.\n",
      "“No,\treally.\tI\t...\n",
      "> Adding chunk: “Don’t\tworry\tabout\tit,”\tI\tsaid.\n",
      "“No,\treally.\tI\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Girls\twere\todd\tcreatures.\n",
      "> Adding chunk: Girls\twere\todd\tcreatures.\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\th...\n",
      "> Adding chunk: 2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\th...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I\thad\tno\tidea\tthey\twere\tso\tlarge,”\tI\tsaid.\n",
      "“Ye...\n",
      "> Adding chunk: “I\thad\tno\tidea\tthey\twere\tso\tlarge,”\tI\tsaid.\n",
      "“Ye...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Do\tI\tjust\tgo\tin?”\tI\tasked.\n",
      "The\tguard\tseemed\tco...\n",
      "> Adding chunk: “Do\tI\tjust\tgo\tin?”\tI\tasked.\n",
      "The\tguard\tseemed\tco...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The\tquestion\tbroke\tthe\tsilence\tand\tI\tsnapped\tmy...\n",
      "> Adding chunk: The\tquestion\tbroke\tthe\tsilence\tand\tI\tsnapped\tmy...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: and\teventually\twe\tturned\ta\tcorner\tand\tthe\tsweet...\n",
      "> Adding chunk: and\teventually\twe\tturned\ta\tcorner\tand\tthe\tsweet...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: around\tin\tmy\tmouth\tto\tease\tthe\tburning.\tIt\twasn...\n",
      "> Adding chunk: around\tin\tmy\tmouth\tto\tease\tthe\tburning.\tIt\twasn...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 3\n",
      "\t\n",
      "“A\tcripple\tthinks\the’ll\tbe\table\tto\tride\ta\td...\n",
      "> Adding chunk: 3\n",
      "\t\n",
      "“A\tcripple\tthinks\the’ll\tbe\table\tto\tride\ta\td...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “How\tdare\tyou\tspeak\tto\tme\tlike\tthat,\tlow\tborn!”...\n",
      "> Adding chunk: “How\tdare\tyou\tspeak\tto\tme\tlike\tthat,\tlow\tborn!”...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Maren\tleaned\tforward\tand\tlowered\ther\tvoice\tto\ta...\n",
      "> Adding chunk: Maren\tleaned\tforward\tand\tlowered\ther\tvoice\tto\ta...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Let’s\tgo,”\tI\tsaid.\n",
      "Maren\tnodded,\tthat\tblasted\t...\n",
      "> Adding chunk: “Let’s\tgo,”\tI\tsaid.\n",
      "Maren\tnodded,\tthat\tblasted\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: but\tI\tcouldn’t\tmake\tout\tthe\twords.\tThe\tmassive\t...\n",
      "> Adding chunk: but\tI\tcouldn’t\tmake\tout\tthe\twords.\tThe\tmassive\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The\texit\tis\tbehind\tus.”\n",
      "“There\tis\talways\tmore\tt...\n",
      "> Adding chunk: The\texit\tis\tbehind\tus.”\n",
      "“There\tis\talways\tmore\tt...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: a\trope\tdown.\n",
      "“Try\tthis,”\tshe\tsaid.\n",
      "I\tgrabbed\tah...\n",
      "> Adding chunk: a\trope\tdown.\n",
      "“Try\tthis,”\tshe\tsaid.\n",
      "I\tgrabbed\tah...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 4\n",
      "\t\n",
      "A\tfew\thours\tlater,\tafter\tI\thad\tcleaned\tthe\t...\n",
      "> Adding chunk: 4\n",
      "\t\n",
      "A\tfew\thours\tlater,\tafter\tI\thad\tcleaned\tthe\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: in\tevery\tdirection\tbefore\tnodding\tto\thimself.\tT...\n",
      "> Adding chunk: in\tevery\tdirection\tbefore\tnodding\tto\thimself.\tT...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: refer\tyourselves\tback\tto\tmy\toriginal\tstatement\t...\n",
      "> Adding chunk: refer\tyourselves\tback\tto\tmy\toriginal\tstatement\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: looked\tcompletely\tdifferent\tfrom\twhat\tit\thad\tju...\n",
      "> Adding chunk: looked\tcompletely\tdifferent\tfrom\twhat\tit\thad\tju...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: couldn’t\thelp\tit.\tI\twas\tparched\tand\tdrank\tsever...\n",
      "> Adding chunk: couldn’t\thelp\tit.\tI\twas\tparched\tand\tdrank\tsever...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 5\n",
      "\t\n",
      "After\tthe\tceremony\twas\tover,\twe\tfollowed\tCu...\n",
      "> Adding chunk: 5\n",
      "\t\n",
      "After\tthe\tceremony\twas\tover,\twe\tfollowed\tCu...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: mastered\tthe\tskills\tnecessary\tto\tkeep\tfrom\tfall...\n",
      "> Adding chunk: mastered\tthe\tskills\tnecessary\tto\tkeep\tfrom\tfall...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: seventh\tbell,\tand\tthe\tthirteenth\tbell.\tIf\tyou\td...\n",
      "> Adding chunk: seventh\tbell,\tand\tthe\tthirteenth\tbell.\tIf\tyou\td...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: was\tneatly\tarranged\ton\tone\tside\tof\tthe\troom.\tI\t...\n",
      "> Adding chunk: was\tneatly\tarranged\ton\tone\tside\tof\tthe\troom.\tI\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: hadn’t\tsaid\tanything.\tHe\twas\ton\this\tbed,\this\tba...\n",
      "> Adding chunk: hadn’t\tsaid\tanything.\tHe\twas\ton\this\tbed,\this\tba...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 6\n",
      "\t\n",
      "I\tmanaged\tto\tget\tdown\tto\tthe\tdining\thall\tin...\n",
      "> Adding chunk: 6\n",
      "\t\n",
      "I\tmanaged\tto\tget\tdown\tto\tthe\tdining\thall\tin...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I\twanted\tto\tapologize\tfor\twhat\tI\tsaid\tlast\tnig...\n",
      "> Adding chunk: “I\twanted\tto\tapologize\tfor\twhat\tI\tsaid\tlast\tnig...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Well\tdone,\tSimon,”\tone\tof\tthe\tguards\tsaid.\t“I\t...\n",
      "> Adding chunk: “Well\tdone,\tSimon,”\tone\tof\tthe\tguards\tsaid.\t“I\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: me.\tI\tcould\tsmell\tthe\tleather\tof\this\tarmor,\talo...\n",
      "> Adding chunk: me.\tI\tcould\tsmell\tthe\tleather\tof\this\tarmor,\talo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:10: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: swing,\twhich\tthrew\this\tmotion\toff.\tThe\tblade\tsw...\n",
      "> Adding chunk: swing,\twhich\tthrew\this\tmotion\toff.\tThe\tblade\tsw...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 7\n",
      "\t\n",
      "When\tI\tawoke,\tI\thad\tno\tidea\twhere\tI\twas.\n",
      "My...\n",
      "> Adding chunk: 7\n",
      "\t\n",
      "When\tI\tawoke,\tI\thad\tno\tidea\twhere\tI\twas.\n",
      "My...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I\twas\tbrowsing\tthe\tvendors.”\n",
      "“How\tdid\tyou\tend\t...\n",
      "> Adding chunk: “I\twas\tbrowsing\tthe\tvendors.”\n",
      "“How\tdid\tyou\tend\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I\thave\tto.\tThis\tis\tmy\tonly\tchance.”\tI\tcouldn’t...\n",
      "> Adding chunk: “I\thave\tto.\tThis\tis\tmy\tonly\tchance.”\tI\tcouldn’t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: have\tto\task.\tDid\tyou\tuse\tmagic\tagainst\tthose\tgu...\n",
      "> Adding chunk: have\tto\task.\tDid\tyou\tuse\tmagic\tagainst\tthose\tgu...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: my\tbody\twas\tsore.\tBy\tthe\ttime\tI\treached\tthe\ttop...\n",
      "> Adding chunk: my\tbody\twas\tsore.\tBy\tthe\ttime\tI\treached\tthe\ttop...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 8\n",
      "\t\n",
      "I\trushed\tinto\tthe\ttemple.\n",
      "To\tmy\trelief,\teve...\n",
      "> Adding chunk: 8\n",
      "\t\n",
      "I\trushed\tinto\tthe\ttemple.\n",
      "To\tmy\trelief,\teve...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: but\tI\tam\tnot\tcompletely\tin\tcontrol\tof\tthe\ttest....\n",
      "> Adding chunk: but\tI\tam\tnot\tcompletely\tin\tcontrol\tof\tthe\ttest....\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Will\tyou\tplease\tjoin\tme\tup\there?”\n",
      "I\trecognized...\n",
      "> Adding chunk: “Will\tyou\tplease\tjoin\tme\tup\there?”\n",
      "I\trecognized...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I’m\tsorry,”\tshe\tsaid.\n",
      "I\tcould\tfeel\tmy\teyes\twel...\n",
      "> Adding chunk: “I’m\tsorry,”\tshe\tsaid.\n",
      "I\tcould\tfeel\tmy\teyes\twel...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: I\twalked\tto\tthe\tfront\tand\tfollowed\tthe\tmaster\tt...\n",
      "> Adding chunk: I\twalked\tto\tthe\tfront\tand\tfollowed\tthe\tmaster\tt...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 9\n",
      "\t\n",
      "I\twas\tin\tAutumnwick’s\tmarket.\n",
      "The\tCitadel\tt...\n",
      "> Adding chunk: 9\n",
      "\t\n",
      "I\twas\tin\tAutumnwick’s\tmarket.\n",
      "The\tCitadel\tt...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: duel\tit\tout\tbetween\tthemselves.\n",
      "“It’s\tthe\tlast\t...\n",
      "> Adding chunk: duel\tit\tout\tbetween\tthemselves.\n",
      "“It’s\tthe\tlast\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: held\tout\ta\tsilver\tcoin\tto\tme.\n",
      "“Please,\ttake\tthi...\n",
      "> Adding chunk: held\tout\ta\tsilver\tcoin\tto\tme.\n",
      "“Please,\ttake\tthi...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: It\twas\tbusiness\tas\tusual.\tWhen\tI\tlooked\tat\tthe\t...\n",
      "> Adding chunk: It\twas\tbusiness\tas\tusual.\tWhen\tI\tlooked\tat\tthe\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: it\twas\ttoo\tstrong.\tI\thad\tno\tidea\twhat\twas\thappe...\n",
      "> Adding chunk: it\twas\ttoo\tstrong.\tI\thad\tno\tidea\twhat\twas\thappe...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Yes,\tbut—”\n",
      "“No,”\the\tinterrupted\tagain,\this\tton...\n",
      "> Adding chunk: “Yes,\tbut—”\n",
      "“No,”\the\tinterrupted\tagain,\this\tton...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 10\n",
      "\t\n",
      "“Gods,”\tI\tbreathed.\n",
      "“What?”\tMaren\tasked.\t“...\n",
      "> Adding chunk: 10\n",
      "\t\n",
      "“Gods,”\tI\tbreathed.\n",
      "“What?”\tMaren\tasked.\t“...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: conversations\twere\tmuted,\tand\teveryone\tseemed\tt...\n",
      "> Adding chunk: conversations\twere\tmuted,\tand\teveryone\tseemed\tt...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Sorry,”\tI\tmumbled.\t“I\twas\tsupposed\tto\tmeet\tyou...\n",
      "> Adding chunk: “Sorry,”\tI\tmumbled.\t“I\twas\tsupposed\tto\tmeet\tyou...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I’ll\ttry\tto,”\tMaren\treplied.\t“Magic\tis\tlike\tth...\n",
      "> Adding chunk: “I’ll\ttry\tto,”\tMaren\treplied.\t“Magic\tis\tlike\tth...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: away\twith\ta\tspell\tthat\tcreated\ta\twall\tor\tsometh...\n",
      "> Adding chunk: away\twith\ta\tspell\tthat\tcreated\ta\twall\tor\tsometh...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 11\n",
      "\t\n",
      "Maren\tstared\tat\tme\tin\tdisbelief.\n",
      "“You\treal...\n",
      "> Adding chunk: 11\n",
      "\t\n",
      "Maren\tstared\tat\tme\tin\tdisbelief.\n",
      "“You\treal...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: then.”\n",
      "I\tknew\tI\tcouldn’t\tlet\ther\tdo\tthat,\tbut\tI...\n",
      "> Adding chunk: then.”\n",
      "I\tknew\tI\tcouldn’t\tlet\ther\tdo\tthat,\tbut\tI...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Perhaps\tsending\ta\tscout\tor\ttwo\tto\tcheck\tthings...\n",
      "> Adding chunk: “Perhaps\tsending\ta\tscout\tor\ttwo\tto\tcheck\tthings...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: doesn’t\teven\tknow\tshe’s\tgone.”\n",
      "“Then\twe\tfeign\ti...\n",
      "> Adding chunk: doesn’t\teven\tknow\tshe’s\tgone.”\n",
      "“Then\twe\tfeign\ti...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: school\tfor\ta\tfew\tdays.\tIt\tseemed\tthat\tmore\tmyst...\n",
      "> Adding chunk: school\tfor\ta\tfew\tdays.\tIt\tseemed\tthat\tmore\tmyst...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 12\n",
      "\t\n",
      "The\tlibrary\twas\tlocated\tat\tthe\tfar\tnorth\te...\n",
      "> Adding chunk: 12\n",
      "\t\n",
      "The\tlibrary\twas\tlocated\tat\tthe\tfar\tnorth\te...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: specific\ttopic,\tyou\tcan\tsearch\tthe\tindex\tfor\tbo...\n",
      "> Adding chunk: specific\ttopic,\tyou\tcan\tsearch\tthe\tindex\tfor\tbo...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: was\tglad\tthat\tmy\tmother\thad\ttaught\tme\tto\tread\ta...\n",
      "> Adding chunk: was\tglad\tthat\tmy\tmother\thad\ttaught\tme\tto\tread\ta...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: lacking\tthe\tbook\tI\tneeded.\n",
      "I\tscratched\tmy\tchin\t...\n",
      "> Adding chunk: lacking\tthe\tbook\tI\tneeded.\n",
      "I\tscratched\tmy\tchin\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: every\tbook\ton\tthe\tFalse\tKing\twas\tinexplicably\tm...\n",
      "> Adding chunk: every\tbook\ton\tthe\tFalse\tKing\twas\tinexplicably\tm...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 13\n",
      "\t\n",
      "“Who\twould\tbe\ttrying\tto\thide\tsomething\trel...\n",
      "> Adding chunk: 13\n",
      "\t\n",
      "“Who\twould\tbe\ttrying\tto\thide\tsomething\trel...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: fine.\tI\tcan\tdo\tit\talone.”\tShe\tstood\tup\tand\tleft...\n",
      "> Adding chunk: fine.\tI\tcan\tdo\tit\talone.”\tShe\tstood\tup\tand\tleft...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: next\tthing\tI\tknew,\tMaren\twas\tjabbing\tme\twith\the...\n",
      "> Adding chunk: next\tthing\tI\tknew,\tMaren\twas\tjabbing\tme\twith\the...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: She\tpushed\ton\tone\tof\tthe\tbricks\tin\tthe\twall\tand...\n",
      "> Adding chunk: She\tpushed\ton\tone\tof\tthe\tbricks\tin\tthe\twall\tand...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: battled\tthe\tboy\tin\tmy\tCompassion\ttest.\tDread\tst...\n",
      "> Adding chunk: battled\tthe\tboy\tin\tmy\tCompassion\ttest.\tDread\tst...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: then\tthat\tthe\tfigure\tfrom\tmy\ttest\tand\tthe\tone\ts...\n",
      "> Adding chunk: then\tthat\tthe\tfigure\tfrom\tmy\ttest\tand\tthe\tone\ts...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Can\tyou\tbreak\tthe\tspell?”\tI\tasked.\n",
      "“No.\tIt’s\tp...\n",
      "> Adding chunk: “Can\tyou\tbreak\tthe\tspell?”\tI\tasked.\n",
      "“No.\tIt’s\tp...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 14\n",
      "\t\n",
      "Master\tPevus\tlistened\tto\tMaren’s\ttale\twith...\n",
      "> Adding chunk: 14\n",
      "\t\n",
      "Master\tPevus\tlistened\tto\tMaren’s\ttale\twith...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “How\tdo\tyou\tknow\tof\tthe\thidden\troom?”\the\tasked....\n",
      "> Adding chunk: “How\tdo\tyou\tknow\tof\tthe\thidden\troom?”\the\tasked....\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (9): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (9): us-api.i.posthog.com:443\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “What\tis\tthat\tstuff?”\tI\tasked.\n",
      "“Charcoal.”\n",
      "“Why...\n",
      "> Adding chunk: “What\tis\tthat\tstuff?”\tI\tasked.\n",
      "“Charcoal.”\n",
      "“Why...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: about\tJosephine,\tso\tit’s\tgoing\tto\tbe\tdifficult\t...\n",
      "> Adding chunk: about\tJosephine,\tso\tit’s\tgoing\tto\tbe\tdifficult\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “What’s\twrong?”\tI\tasked.\n",
      "“I\tcan’t\thelp\tbut\tthin...\n",
      "> Adding chunk: “What’s\twrong?”\tI\tasked.\n",
      "“I\tcan’t\thelp\tbut\tthin...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 15\n",
      "\t\n",
      "When\tthe\tbell\trang\tfor\tbreakfast\tin\tthe\tmo...\n",
      "> Adding chunk: 15\n",
      "\t\n",
      "When\tthe\tbell\trang\tfor\tbreakfast\tin\tthe\tmo...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: you’re\tright.”\n",
      "Curate\tAnesko\tcleared\this\tthroat...\n",
      "> Adding chunk: you’re\tright.”\n",
      "Curate\tAnesko\tcleared\this\tthroat...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Anesko\tordered\tus\tto\tsit\tand\twe\tobeyed.\tThe\twal...\n",
      "> Adding chunk: Anesko\tordered\tus\tto\tsit\tand\twe\tobeyed.\tThe\twal...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: eyes.\tThe\twind\tfelt\tamazing\tagainst\tmy\tface.\tI\t...\n",
      "> Adding chunk: eyes.\tThe\twind\tfelt\tamazing\tagainst\tmy\tface.\tI\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Oh.\tThat’s\tinteresting.\tWhat\tabout\tgreen\tones?...\n",
      "> Adding chunk: “Oh.\tThat’s\tinteresting.\tWhat\tabout\tgreen\tones?...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: looked\tat\tit,\tthen\tput\tit\tto\tmy\tmouth\tand\tlifte...\n",
      "> Adding chunk: looked\tat\tit,\tthen\tput\tit\tto\tmy\tmouth\tand\tlifte...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 16\n",
      "\t\n",
      "Magic\thummed\tin\tmy\tears.\n",
      "It\twasn’t\toverly\t...\n",
      "> Adding chunk: 16\n",
      "\t\n",
      "Magic\thummed\tin\tmy\tears.\n",
      "It\twasn’t\toverly\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: magic.\n",
      "And\tthen\tthe\tresistance\twas\tgone\tcomplet...\n",
      "> Adding chunk: magic.\n",
      "And\tthen\tthe\tresistance\twas\tgone\tcomplet...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: continued\tmoving.\tThe\tcreature\tdidn’t\tstop.\tIt\t...\n",
      "> Adding chunk: continued\tmoving.\tThe\tcreature\tdidn’t\tstop.\tIt\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: After\ta\tfew\tmoments\tof\tpanic,\tthe\tweakness\twent...\n",
      "> Adding chunk: After\ta\tfew\tmoments\tof\tpanic,\tthe\tweakness\twent...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Pevus\tnor\tthe\tCurates\twould\thave\tallowed\tsuch\ta...\n",
      "> Adding chunk: Pevus\tnor\tthe\tCurates\twould\thave\tallowed\tsuch\ta...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: gargoyle’s\tbroken\tbody\tand\tcouldn’t\thelp\tbut\two...\n",
      "> Adding chunk: gargoyle’s\tbroken\tbody\tand\tcouldn’t\thelp\tbut\two...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 17\n",
      "\t\n",
      "I\twas\tsitting\tin\ta\tsmall\troom\toutside\tMast...\n",
      "> Adding chunk: 17\n",
      "\t\n",
      "I\twas\tsitting\tin\ta\tsmall\troom\toutside\tMast...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: That\tleaves\tonly\tone\texplanation.”\n",
      "I\twaited\tfor...\n",
      "> Adding chunk: That\tleaves\tonly\tone\texplanation.”\n",
      "I\twaited\tfor...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: deal\twith\tthe\tFalse\tKing.\tNow,\tyou\tshould\tget\ts...\n",
      "> Adding chunk: deal\twith\tthe\tFalse\tKing.\tNow,\tyou\tshould\tget\ts...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: midnight\twhen\tI\tclimbed\tinto\tmy\tbed.\tI\tdrifted\t...\n",
      "> Adding chunk: midnight\twhen\tI\tclimbed\tinto\tmy\tbed.\tI\tdrifted\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: likely\tto\tbe\tlate\tand\tthen\t…”\tI\ttrailed\toff\twhe...\n",
      "> Adding chunk: likely\tto\tbe\tlate\tand\tthen\t…”\tI\ttrailed\toff\twhe...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: waiting\tfor\tthe\trest\tof\this\tinstructions.\n",
      "“Well...\n",
      "> Adding chunk: waiting\tfor\tthe\trest\tof\this\tinstructions.\n",
      "“Well...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 18\n",
      "\t\n",
      "By\tthe\ttime\tI’d\tmade\tit\taround\tthe\tentire\t...\n",
      "> Adding chunk: 18\n",
      "\t\n",
      "By\tthe\ttime\tI’d\tmade\tit\taround\tthe\tentire\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: intensely.\tI\thung\tin\tplace\tand\tmy\tarms\twere\tsha...\n",
      "> Adding chunk: intensely.\tI\thung\tin\tplace\tand\tmy\tarms\twere\tsha...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Let\tme\tsay\tthis:\tif\tyou\tare\tinjured\tto\tthe\tpoi...\n",
      "> Adding chunk: “Let\tme\tsay\tthis:\tif\tyou\tare\tinjured\tto\tthe\tpoi...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “I\tlearned\thow\tto\tdefend\tmyself\tfrom\tthe\tcaptai...\n",
      "> Adding chunk: “I\tlearned\thow\tto\tdefend\tmyself\tfrom\tthe\tcaptai...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: brutal\tthings\tthan\twhat\tthis\ttest\toffered.\tStil...\n",
      "> Adding chunk: brutal\tthings\tthan\twhat\tthis\ttest\toffered.\tStil...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 19\n",
      "\t\n",
      "It\tonly\ttook\tme\ta\tmoment\tto\tdecide\tthat\tI\t...\n",
      "> Adding chunk: 19\n",
      "\t\n",
      "It\tonly\ttook\tme\ta\tmoment\tto\tdecide\tthat\tI\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: made\tthings\tworse\tbecause\tof\tthe\tmud\ton\tthe\tgar...\n",
      "> Adding chunk: made\tthings\tworse\tbecause\tof\tthe\tmud\ton\tthe\tgar...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “By\taccident\t…\tbut\tthe\topen\tdoor\tof\tthe\tbuildin...\n",
      "> Adding chunk: “By\taccident\t…\tbut\tthe\topen\tdoor\tof\tthe\tbuildin...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Yes.”\n",
      "“And\tyou\tcame\there\tto\thelp\ther\teven\tthou...\n",
      "> Adding chunk: “Yes.”\n",
      "“And\tyou\tcame\there\tto\thelp\ther\teven\tthou...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: wouldn’t.\tShe\twas\ttoo\tconflicted\twith\therself\tt...\n",
      "> Adding chunk: wouldn’t.\tShe\twas\ttoo\tconflicted\twith\therself\tt...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 20\n",
      "\t\n",
      "I\tdidn’t\tknow\thow\tmuch\ttime\thad\tpassed\twhe...\n",
      "> Adding chunk: 20\n",
      "\t\n",
      "I\tdidn’t\tknow\thow\tmuch\ttime\thad\tpassed\twhe...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Master\tPevus\tseemed\tlike\the\twas\tabout\tto\tobject...\n",
      "> Adding chunk: Master\tPevus\tseemed\tlike\the\twas\tabout\tto\tobject...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: and\talmost\tlost\tmy\tgrip\ton\tthe\thilt.\tAlthough\tm...\n",
      "> Adding chunk: and\talmost\tlost\tmy\tgrip\ton\tthe\thilt.\tAlthough\tm...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: As\tsoon\tas\tSebastian\tturned\n",
      "his\tback\tto\tme,\tI\ts...\n",
      "> Adding chunk: As\tsoon\tas\tSebastian\tturned\n",
      "his\tback\tto\tme,\tI\ts...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: his\tblade.\tI\twent\tafter\thim,\tbut\the\twas\tquicker...\n",
      "> Adding chunk: his\tblade.\tI\twent\tafter\thim,\tbut\the\twas\tquicker...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: how\tI\twould\tescape\tthe\tCitadel\tbefore\tMaster\tPe...\n",
      "> Adding chunk: how\tI\twould\tescape\tthe\tCitadel\tbefore\tMaster\tPe...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 21\n",
      "\t\n",
      "The\tnext\tmorning,\tI\tawoke\tbleary-eyed\tand\t...\n",
      "> Adding chunk: 21\n",
      "\t\n",
      "The\tnext\tmorning,\tI\tawoke\tbleary-eyed\tand\t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: were\tbags\tunder\this\teyes,\tand\tI\tguessed\tthat\the...\n",
      "> Adding chunk: were\tbags\tunder\this\teyes,\tand\tI\tguessed\tthat\the...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: “Yes,\tMaster.\tIt\tdoes\tseem\todd\tto\tme\tthat\twe’re...\n",
      "> Adding chunk: “Yes,\tMaster.\tIt\tdoes\tseem\todd\tto\tme\tthat\twe’re...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: be\thard\tto\ttell\tin\tthis\tlight.”\n",
      "“Hello\tPhlandyr...\n",
      "> Adding chunk: be\thard\tto\ttell\tin\tthis\tlight.”\n",
      "“Hello\tPhlandyr...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: locked\tin\ta\troom\tsomewhere,\tnot\tout\tin\tthe\topen...\n",
      "> Adding chunk: locked\tin\ta\troom\tsomewhere,\tnot\tout\tin\tthe\topen...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Time\twas\tmy\tenemy.\tI\tturned\tand\tswam\tfor\tthe\tsh...\n",
      "> Adding chunk: Time\twas\tmy\tenemy.\tI\tturned\tand\tswam\tfor\tthe\tsh...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The\tjourney\tcontinues\twith\tA\tBond\tof\tFlame,\tava...\n",
      "> Adding chunk: The\tjourney\tcontinues\twith\tA\tBond\tof\tFlame,\tava...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: OTHER\tBOOKS\n",
      "\t\n",
      "Hey\tthere!\n",
      "I\twrite\tfantasy\tand\tsp...\n",
      "> Adding chunk: OTHER\tBOOKS\n",
      "\t\n",
      "Hey\tthere!\n",
      "I\twrite\tfantasy\tand\tsp...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Shard\tof\tthe\tSun\n",
      "\t\n",
      "SPACE\tOPERA\n",
      "Galactic\tMercena...\n",
      "> Adding chunk: Shard\tof\tthe\tSun\n",
      "\t\n",
      "SPACE\tOPERA\n",
      "Galactic\tMercena...\n",
      "INFO:backoff:Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F35B2FB50>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F35B2FB50>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (10): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (10): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.1s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F34DF5450>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.1s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F34DF5450>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (11): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (11): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 2.5s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F35B4CF10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 2.5s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F35B4CF10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (12): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (12): us-api.i.posthog.com:443\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F35AD0150>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F35AD0150>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:chromadb.config:Starting component LocalHnswSegment\n",
      "Starting component LocalHnswSegment\n",
      "CPU times: total: 32min 46s\n",
      "Wall time: 4min 37s\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (13): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (13): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F16A28AD0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F16A28AD0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (14): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (14): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 1.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33AACFD0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 1.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33AACFD0>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (15): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (15): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F335EA190>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F335EA190>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (16): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (16): us-api.i.posthog.com:443\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33B9FC10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F33B9FC10>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, download_loader\n",
    "\n",
    "PDFReader = download_loader('PDFReader')\n",
    "loader = PDFReader()\n",
    "\n",
    "documents = loader.load_data(file=Path(\"./pdfs/Trial-by-Sorcery.pdf\"))\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_model,\n",
    "    system_prompt='You are a bot that answers questions about the book.'\n",
    ")\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
    "chroma_client.delete_collection(\"quickstart\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
    "    \n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store2 = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context2 = StorageContext.from_defaults(vector_store=vector_store2)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context2, embed_model=embedding_model, service_context=service_context\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4822cbb-1dc4-447a-aec5-bbf9f2ecbd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.vector_stores.chroma.base:> Top 1 nodes:\n",
      "> Top 1 nodes:\n",
      "DEBUG:llama_index.vector_stores.chroma.base:> [Node 052103da-46a0-4e66-a7ab-a0f001ddfe30] [Similarity score: 0.5267347034731992] 2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\theavily\tguarded\tthan\tthe\tcity\n",
      "gates.\tAnd\tthese\tguar...\n",
      "> [Node 052103da-46a0-4e66-a7ab-a0f001ddfe30] [Similarity score: 0.5267347034731992] 2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\theavily\tguarded\tthan\tthe\tcity\n",
      "gates.\tAnd\tthese\tguar...\n",
      "DEBUG:llama_index.vector_stores.chroma.base:> [Node b219b40b-73bb-4197-93a5-34f27b6c5be1] [Similarity score: 0.46287886124050837] 1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "It\twas\thome\tto\tthe\tDragon\tGuard,\tthe\tgreatest\twarr...\n",
      "> [Node b219b40b-73bb-4197-93a5-34f27b6c5be1] [Similarity score: 0.46287886124050837] 1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "It\twas\thome\tto\tthe\tDragon\tGuard,\tthe\tgreatest\twarr...\n",
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node 052103da-46a0-4e66-a7ab-a0f001ddfe30] [Similarity score:             0.526735] 2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\theavily\tguarded\tthan\tthe\tcity\n",
      "gates.\tAnd\tthese\tguar...\n",
      "> [Node b219b40b-73bb-4197-93a5-34f27b6c5be1] [Similarity score:             0.462879] 1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "It\twas\thome\tto\tthe\tDragon\tGuard,\tthe\tgreatest\twarr...\n",
      "> Top 2 nodes:\n",
      "> [Node 052103da-46a0-4e66-a7ab-a0f001ddfe30] [Similarity score:             0.526735] 2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\theavily\tguarded\tthan\tthe\tcity\n",
      "gates.\tAnd\tthese\tguar...\n",
      "> [Node b219b40b-73bb-4197-93a5-34f27b6c5be1] [Similarity score:             0.462879] 1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "It\twas\thome\tto\tthe\tDragon\tGuard,\tthe\tgreatest\twarr...\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (17): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (17): us-api.i.posthog.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F26916D50>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.4s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F26916D50>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (18): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (18): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.6s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F2F345950>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.6s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F2F345950>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (19): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (19): us-api.i.posthog.com:443\n",
      "INFO:backoff:Backing off send_request(...) for 0.8s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F34E0BF50>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Backing off send_request(...) for 0.8s (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F34E0BF50>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (20): us-api.i.posthog.com:443\n",
      "Starting new HTTPS connection (20): us-api.i.posthog.com:443\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F29EDE590>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n",
      "Giving up send_request(...) after 4 tries (requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000024F29EDE590>, 'Connection to us-api.i.posthog.com timed out. (connect timeout=15)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  103084.24 ms\n",
      "llama_print_timings:      sample time =      17.15 ms /    73 runs   (    0.23 ms per token,  4256.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  378241.80 ms /  1882 tokens (  200.98 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:        eval time =   26945.30 ms /    72 runs   (  374.24 ms per token,     2.67 tokens per second)\n",
      "llama_print_timings:       total time =  405725.66 ms /  1954 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prompt: **\n",
      "You are a bot that answers questions about podcast transcripts\n",
      "\n",
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 11\n",
      "file_name: pdfs\\Trial-by-Sorcery.pdf\n",
      "\n",
      "2\n",
      "\t\n",
      "The\tentrance\tto\tthe\tCitadel\twas\tmuch\tmore\theavily\tguarded\tthan\tthe\tcity\n",
      "gates.\tAnd\tthese\tguards\tweren’t\tthe\tcity\tguard,\teither.\tThey\twere\tDragon\n",
      "Guards.\tTheir\tarmor\twas\tdecorated\tto\tlook\tlike\tdragon\tscales,\tbut\tit\twas\n",
      "versatile\tand\tpractical\tfor\tbattle.\tBehind\tthe\tassemblage\tof\tguards\twas\ta\tlong\n",
      "wooden\ttable\tthat\thad\tweapons\tscattered\thaphazardly\ton\tits\tsurface.\n",
      "As\tI\tdrew\tnearer,\tthere\twas\ta\t\n",
      "whooshing\n",
      "\tsound\tthat\techoed\toff\tthe\tmassive\n",
      "walls\tand\tmade\tthe\titems\ton\tthe\ttable\tclatter.\tThe\tguards\tseemed\n",
      "unperturbed\tby\tthe\tnoise,\tbut\tI\twas\ttrying\tto\tfigure\tout\twhat\tit\twas\tand\n",
      "where\tit\twas\tcoming\tfrom.\tSuddenly,\ta\tmassive\tblue\tdragon\tswooped\tdown\n",
      "from\tthe\tsky\tand\tlanded\tin\tthe\tcourtyard.\n",
      "I\theld\tmy\tbreath\tin\tawe\tas\tI\tstared\tat\tthe\tpowerful\tbeast.\tIt\twas\teasily\n",
      "thirty\tfeet\tlong\tfrom\tits\tnose\tto\tits\ttail.\tThe\tdragon’s\trider\tslid\toff\tthe\tbeast’s\n",
      "back\tfrom\tthe\tshoulder\tand\tlanded\ton\tthe\tground\tgracefully.\tI\tsnapped\tmy\n",
      "mouth\tclosed\tand\tblinked\tseveral\ttimes.\tIn\tall\tthe\tyears\tmy\tfather\thad\tbeen\ta\n",
      "dragoon,\tI\tnever\thad\tthe\tchance\tto\tsee\this\tdragon.\tAside\tfrom\twhen\tthe\n",
      "guards\ttraveled\tthe\tkingdom,\tdragons\twere\tto\tbe\tkept\tat\tthe\tCitadel\tunder\n",
      "lock\tand\tkey.\tI\tdidn’t\tknow\twhy,\tthough.\n",
      "Now\tthat\tI\twas\tstanding\tbefore\ta\tdragon,\tI\tcould\thardly\tfathom\thow\tbig\tit\n",
      "was.\tIts\tshoulder\twas\tsix\tfeet\tabove\tthe\tground\tand\tits\twingspan\twas\n",
      "massive.\tI\ttried\tto\teyeball\tthe\tlength,\tbut\tit\thad\tto\tbe\talmost\ta\thundred\tfeet\n",
      "across.\tMy\tfocus\ton\tthe\tdragon\twas\tbroken\tas\tthe\tguards\tgot\tmy\tattention.\n",
      "“Hey\tthere,”\tone\tof\tthem\tcalled\tout.\t“Step\tforward.”\n",
      "I\tdid\tas\the\tasked\tand\twalked\tcloser,\tbut\tmy\tgaze\tremained\tlocked\ton\tthe\n",
      "dragon.\tAn\tolder\tman\tapproached\tthe\tcreature\tand\ttook\tits\treins,\tthen\tled\tit\n",
      "around\tthe\tback\tof\tthe\tcastle.\tWith\ta\tswish\tof\tits\ttail,\tthe\tdragon\tdisappeared\n",
      "behind\tthe\tfortress\tand\tI\tlooked\tat\tthe\tguard\twho’d\tspoken.\n",
      "“First\ttime,\thuh?”\the\tgrinned.\t“I\tremember\tmy\tfirst\ttime\tseeing\ta\tdragon,\n",
      "too.\tIt’s\tsomething\tyou\tnever\tforget.”\n",
      "\n",
      "page_label: 5\n",
      "file_name: pdfs\\Trial-by-Sorcery.pdf\n",
      "\n",
      "1\n",
      "\t\n",
      "I\tmarveled\tat\tthe\tvastness\tof\tthe\tCitadel.\n",
      "It\twas\thome\tto\tthe\tDragon\tGuard,\tthe\tgreatest\twarriors\tof\tthe\tkingdom.\n",
      "While\tthat\twas\timpressive\talone,\tit\twas\tmade\teven\tmore\tamazing\tbecause\tit\n",
      "was\talso\tthe\thome\tof\tdragons.\tThe\tmassive,\tpowerful\tcreatures\twere\tkept\tin\n",
      "the\tlower\tchamber\tof\tthe\tcastle.\tAt\tleast,\tthat’s\twhat\tmy\tfather\tused\tto\ttell\n",
      "me.\n",
      "A\twall\tforty\tfeet\thigh\tsurrounded\tthe\tcity\tof\tAutumnwick,\tas\twell\tas\tthe\n",
      "stone\tfortress\tthat\ttowered\tbehind\tit.\tThis\twas\tmy\tfirst\ttime\tseeing\tthe\tplace,\n",
      "and\tit\twas\tjust\tas\tlarge\tand\timposing\tas\tI’d\talways\timagined\tit\tto\tbe.\tThe\n",
      "massive\tgates\tthat\tprovided\tentrance\tthrough\tthe\twall\twere\tmanned\twith\n",
      "guards\tarmed\tto\tthe\tteeth.\tA\tsmall\tline\thad\tformed\tat\tthe\tentrance\tas\tthe\n",
      "guards\tchecked\teveryone\tentering.\n",
      "I\ttraveled\tdownhill\tand\tjoined\tthe\tline,\tadjusting\tmy\tsword\tbelt.\tThe\n",
      "weight\tof\tthe\tblade\tcontinuously\tpulled\tdown\ton\tmy\tpants.\tIt\tmade\tme\n",
      "reconsider\tmy\tdecision\tto\tuse\ta\tside\tsheath\tinstead\tof\tone\tthat\twent\tover\tthe\n",
      "shoulder.\tIt\twas\ttoo\tlate\tto\tchange\tmy\tmind\tnow.\tI’d\tspent\tthe\tlast\tof\tmy\n",
      "coins\tto\treach\tthe\tCitadel,\tand\tI\tdoubted\tthe\tschool\twould\tallow\tme\tto\tcarry\n",
      "a\tblade\tduring\tmy\ttraining\tanyway.\n",
      "The\tline\tshuffled\tforward\tslowly.\tI\tdid\tmy\tbest\tto\tremain\tpatient,\tbut\tit\n",
      "was\tdifficult.\tI\twas\tfinally\there!\tThe\thome\tof\tthe\tDragon\tGuard!\tI’d\tdreamed\n",
      "of\tjoining\ttheir\tranks\tfor\tas\tlong\tas\tI\tcould\tremember.\tMy\tfather’s\tstories\thad\n",
      "always\tbeen\tfilled\twith\tawe\tand\twonder\tas\the\tdescribed\this\tdragon\tand\tthe\n",
      "bond\tthey\tshared.\n",
      "Although\tit\twas\tstill\tearly\tin\tthe\tday,\tthe\tsky\twas\tclear\tand\tthe\tsun\tbeat\n",
      "down\tmercilessly.\tI\tcould\tfeel\tdroplets\tof\tsweat\trunning\tdown\tmy\tback\tand\n",
      "sides.\tI\tdrank\tthe\tlast\tof\tthe\twater\tin\tmy\tcanteen\tand\tcontinued\tto\twait.\tAfter\n",
      "what\tfelt\tlike\tan\teternity\tof\tbaking\tin\tthe\tsun,\tI\twas\tnext\tfor\tinspection.\tI\n",
      "glanced\tbehind\tme\tand\tsaw\tthe\tline\twas\tmuch\tlonger\tnow.\tThere\twere\tat\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What is a Dragon Guard?\n",
      "Answer: \n",
      "**************************************************\n",
      "** Completion: **\n",
      "\n",
      "A Dragon Guard is a warrior from the kingdom who is part of the Dragoon unit, which is the greatest fighting force in the kingdom. They are heavily guarded and their armor is decorated to look like dragon scales, but it is versatile and practical for battle. They are based in the Citadel, which is also home to dragons.\n",
      "**************************************************\n",
      "\n",
      "\n",
      "CPU times: total: 37min 27s\n",
      "Wall time: 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = query_engine.query(\"What is a Dragon Guard?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67967d5-dd4a-4fa5-a1f5-a4dce571bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from ./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q4_K:  833 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 24.62 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors:        CPU buffer size = 25215.87 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32000\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    71.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2052.03 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n",
      "<timed exec>:22: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 45s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, download_loader\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import ServiceContext\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "PDFReader = download_loader('PDFReader')\n",
    "loader = PDFReader()\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_path=\"./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",  # https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\n",
    "    context_window=32000,\n",
    "    max_new_tokens=768,\n",
    "    # model_kwargs={'n_gpu_layers': 1},\n",
    "    verbose=True\n",
    ")\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"intfloat/e5-base-v2\") \n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_model,\n",
    "    system_prompt='You are a bot that answers questions about the book.'\n",
    ")\n",
    "\n",
    "documents = loader.load_data(file=Path(\"./pdfs/Trial-by-Sorcery.pdf\"))\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart2\")\n",
    "chroma_client.delete_collection(\"quickstart2\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart2\")\n",
    "                                        \n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store2 = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context2 = StorageContext.from_defaults(vector_store=vector_store2)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context2, embed_model=embedding_model, service_context=service_context\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f81c02c-f77a-44f2-bdb0-94e88f45494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  144827.68 ms\n",
      "llama_print_timings:      sample time =       5.41 ms /    15 runs   (    0.36 ms per token,  2773.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   90902.54 ms /   181 tokens (  502.22 ms per token,     1.99 tokens per second)\n",
      "llama_print_timings:        eval time =   31097.30 ms /    14 runs   ( 2221.24 ms per token,     0.45 tokens per second)\n",
      "llama_print_timings:       total time =  122168.07 ms /   195 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 21s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#response = query_engine.query(\"query: What is a Dragon Guard?\")\n",
    "response = query_engine.query(\"query: Who is the author of the book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1ae3f3-e2ed-44a9-a0e7-4cd750c2ae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The context information provided does not contain any details about what a Dragon Guard is.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
